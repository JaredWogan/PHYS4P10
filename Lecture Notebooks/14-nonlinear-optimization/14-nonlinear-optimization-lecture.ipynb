{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 14 - Nonlinear optimization\n",
    "\n",
    "So far we have looked at methods to solve single equations of a single variable, such as\n",
    "$$x=f(x)\\ \\ \\textrm{ and } f(x)=0$$\n",
    "\n",
    "We need to now extend these methods to solve equations involving multiple variables, such as\n",
    "$$x=f(x,y), \\ y=g(x,y),$$\n",
    "or\n",
    "$$\\begin{array}{c}\n",
    "x_1 &=& f_1(x_1,\\dots,x_N), \\\\\n",
    " & \\vdots & \\\\\n",
    "x_N &=& f_N(x_1,\\dots,x_N)\n",
    "\\end{array}$$\n",
    "or \n",
    "$$\\begin{array}{c}\n",
    "f_1(x_1,\\dots,x_N) &=& 0, \\\\\n",
    " \\vdots & \\\\\n",
    "f_N(x_1,\\dots,x_N) &=& 0\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import scipy.misc\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relaxation method\n",
    "\n",
    "The relaxation method can be extended easily to multiple variables. Just as we saw with one variable, it is important to try different ways of writing the equations to see how they converge to a solution.\n",
    "\n",
    "#### Example 1\n",
    "Determine the intersection point between a line and a parabola. The equations are $$ y=ax+b,\\ y=x^2+c.$$ There are two ways of solving one equation for $x$ and the other for $y$.\n",
    "$$x'=\\frac{y-b}{a}, \\ y' = x^2+c$$\n",
    "or\n",
    "$$x' = \\sqrt{y-c}, \\ y'= ax+b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :   1.04881 Y :  2.90000\n",
      "X :   1.37840 Y :  2.04881\n",
      "X :   1.02411 Y :  2.37840\n",
      "X :   1.17405 Y :  2.02411\n",
      "X :   1.01199 Y :  2.17405\n",
      "X :   1.08354 Y :  2.01199\n",
      "X :   1.00597 Y :  2.08354\n",
      "X :   1.04093 Y :  2.00597\n",
      "X :   1.00298 Y :  2.04093\n",
      "X :   1.02026 Y :  2.00298\n",
      "X :   1.00149 Y :  2.02026\n",
      "X :   1.01008 Y :  2.00149\n",
      "X :   1.00074 Y :  2.01008\n",
      "X :   1.00503 Y :  2.00074\n",
      "X :   1.00037 Y :  2.00503\n",
      "X :   1.00251 Y :  2.00037\n",
      "X :   1.00019 Y :  2.00251\n",
      "X :   1.00125 Y :  2.00019\n",
      "X :   1.00009 Y :  2.00125\n",
      "X :   1.00063 Y :  2.00009\n",
      "X :   1.00005 Y :  2.00063\n",
      "X :   1.00031 Y :  2.00005\n",
      "X :   1.00002 Y :  2.00031\n",
      "X :   1.00016 Y :  2.00002\n",
      "X :   1.00001 Y :  2.00016\n",
      "X :   1.00008 Y :  2.00001\n",
      "X :   1.00001 Y :  2.00008\n",
      "X :   1.00004 Y :  2.00001\n",
      "X :   1.00000 Y :  2.00004\n",
      "X :   1.00002 Y :  2.00000\n",
      "X :   1.00000 Y :  2.00002\n",
      "X :   1.00001 Y :  2.00000\n",
      "X :   1.00000 Y :  2.00001\n",
      "X :   1.00000 Y :  2.00000\n",
      "X :   1.00000 Y :  2.00000\n",
      "X :   1.00000 Y :  2.00000\n",
      "X :   1.00000 Y :  2.00000\n",
      "X :   1.00000 Y :  2.00000\n",
      "X :   1.00000 Y :  2.00000\n",
      "X :   1.00000 Y :  2.00000\n",
      "X :   1.00000 Y :  2.00000\n"
     ]
    }
   ],
   "source": [
    "# -- I am using our new data container; the tuple.\n",
    "# -- Tuples are unchangable lists that are used for \n",
    "# -- moving collection of data from place to place in your code.\n",
    "# --\n",
    "# -- Tuples use (,). Lists use [,]\n",
    "# --\n",
    "# -- With these values of a,b,c there should be two intersections\n",
    "# -- at (1,2) and (0,1)\n",
    "constants = ( 1.0, # a\n",
    "              1.0, # b\n",
    "              1.0) # c\n",
    "# x = 0.9 # USE f0 to find intersection at (0,1)\n",
    "# y = 0.1\n",
    "x = 1.9  # USE f1  to find intersection at (1,2)\n",
    "y = 2.1\n",
    "\n",
    "# -- See how the varible args will contain the \n",
    "# -- tuple 'constants', and is unpacked to reveal a,b,c\n",
    "# -- This way I need only to change a,b,c in only one place and re-run the program.\n",
    "def f0(x,y,args) :\n",
    "    a,b,c = args\n",
    "    xp = (y-b)/a\n",
    "    yp = x*x+c\n",
    "    return xp,yp \n",
    "\n",
    "def f1(x,y,args) :\n",
    "    a,b,c = args\n",
    "    xp = numpy.sqrt(y-c)\n",
    "    yp = a*x+b\n",
    "    return xp,yp \n",
    "\n",
    "# --- Here is the relaxation loop.\n",
    "for i in range(100):\n",
    "    xp,yp = f1(x,y,constants)\n",
    "    if (numpy.isclose(x,xp,atol=1.E-6,rtol=0.)) and (numpy.isclose(y,yp,atol=1.E-6,rtol=0.)) :\n",
    "        print(\"X : {0:9.5f} Y : {1:8.5f}\".format(x,y))\n",
    "        break\n",
    "    else :\n",
    "        x = xp\n",
    "        y = yp\n",
    "        print(\"X : {0:9.5f} Y : {1:8.5f}\".format(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's method.\n",
    "\n",
    "Newtown's method can be extended in multiple ways to solve for simultaneous nonlinear equations.\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "f_1(x_1,\\dots,x_N) &=& 0, \\\\\n",
    " \\vdots & \\\\\n",
    "f_N(x_1,\\dots,x_N) &=& 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We take a Taylor expansion of *each function*, around *each varaible's* inital guess point \n",
    "$\\left\\{x^*_1,\\dots,x^*_N\\right\\}$\n",
    "$$\n",
    "f_i(x_1,\\dots,x_N) = f_i(x^*_1,\\dots,x^*_N)+\\sum_j(x_j-x^*_j)\\frac{\\partial f_i}{\\partial x_j}\\biggr\\rvert_{x^*}+\\dots\n",
    "$$\n",
    "\n",
    "This is a matrix equation\n",
    "$$\n",
    "\\vec{f}(\\vec{x})=\\vec{f}(\\vec{x}^*)+\\pmb{J}\\cdot(\\vec{x}-\\vec{x}^*)+\\dots\n",
    "$$\n",
    "where $\\pmb{J}$ is the Jacobian matrix $J_{ij}=\\partial f_i/\\partial x_j|_{x^*}$\n",
    "\n",
    "As we approach the root, the left hand side becomes zero, and if we define the step of our guess twards the solution as $\\Delta\\vec{x}=\\vec{x}^*-\\vec{x}$ we have\n",
    "$$\n",
    "\\pmb{J}\\cdot\\Delta\\vec{x}=\\vec{f}\n",
    "$$ \n",
    "and we can solve for $\\Delta\\vec{x}$ using linear algebra. A sinlge step in Newton's method is to move the inital guess to a new point\n",
    "$$\n",
    "\\vec{x}'=\\vec{x}^*-\\Delta\\vec{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2\n",
    "Consider the following circuit\n",
    "<img src=\"./images/nlcircuit.png\" width=200/>\n",
    "The resistors obey Ohm's law, but the diode obeys the <a href=\"https://en.wikipedia.org/wiki/Shockley_diode_equation\">diode equation</a>\n",
    "$$I = I_0\\left(e^{V/V_T}-1\\right)$$\n",
    "where $V$ is the voltage across the diode, and $I_0$ and $V_T$ are constants. Applying the Kirchoff current law we have at points $V_1$ and $V_2$,\n",
    "$$f_1 = \\frac{V_1+V_+}{R_1} + \\frac{V_1-0}{R_2} + I_0\\left(e^{(V_1-V_2)/V_T}-1\\right)=0$$\n",
    "$$f_2 = \\frac{V_2+V_+}{R_3} + \\frac{V_2-0}{R_4} - I_0\\left(e^{(V_1-V_2)/V_T}-1\\right)=0$$\n",
    "The first term of the Jacobian is\n",
    "$$\\frac{\\partial f_1}{\\partial V_1} = \\frac{1}{R_1} + \\frac{1}{R_2} + \\frac{I_0}{V_T}e^{(V_1-V_2)/V_T}$$\n",
    "I leave it to the reader to calculate the remaining 3 terms.\n",
    "\n",
    "Use Newton's method to fing $V_1$ and $V_2$ given\n",
    "$$\\begin{array}{c}\n",
    "V_+ &=& 5\\textrm{ V} & V_T &=& 0.05\\textrm{ V}\\\\\n",
    "R_1 &=& 1\\textrm{ k$\\Omega$} & R_2 &=& 4\\textrm{ k$\\Omega$} & R_3 &=& 3\\textrm{ k$\\Omega$} & R_4 &=& 2\\textrm{ k$\\Omega$}\\\\\n",
    "I_0 &=& 3\\textrm{ nA}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :    3.9999 V     Y :   2.0001 V\n",
      "X :    3.9799 V     Y :   2.0301 V\n",
      "X :    3.9599 V     Y :   2.0601 V\n",
      "X :    3.9399 V     Y :   2.0901 V\n",
      "X :    3.9199 V     Y :   2.1201 V\n",
      "X :    3.8999 V     Y :   2.1501 V\n",
      "X :    3.8799 V     Y :   2.1801 V\n",
      "X :    3.8599 V     Y :   2.2101 V\n",
      "X :    3.8399 V     Y :   2.2401 V\n",
      "X :    3.8199 V     Y :   2.2701 V\n",
      "X :    3.7999 V     Y :   2.3001 V\n",
      "X :    3.7799 V     Y :   2.3301 V\n",
      "X :    3.7599 V     Y :   2.3601 V\n",
      "X :    3.7399 V     Y :   2.3901 V\n",
      "X :    3.7199 V     Y :   2.4201 V\n",
      "X :    3.6999 V     Y :   2.4501 V\n",
      "X :    3.6799 V     Y :   2.4801 V\n",
      "X :    3.6599 V     Y :   2.5101 V\n",
      "X :    3.6399 V     Y :   2.5401 V\n",
      "X :    3.6199 V     Y :   2.5701 V\n",
      "X :    3.5999 V     Y :   2.6001 V\n",
      "X :    3.5799 V     Y :   2.6301 V\n",
      "X :    3.5599 V     Y :   2.6601 V\n",
      "X :      3.54 V     Y :     2.69 V\n",
      "X :    3.5202 V     Y :   2.7198 V\n",
      "X :    3.5006 V     Y :   2.7491 V\n",
      "X :    3.4819 V     Y :   2.7771 V\n",
      "X :    3.4653 V     Y :   2.8021 V\n",
      "X :    3.4532 V     Y :   2.8202 V\n",
      "X :    3.4478 V     Y :   2.8283 V\n",
      "X :     3.447 V     Y :   2.8295 V\n",
      "X :     3.447 V     Y :   2.8296 V\n",
      "X :     3.447 V     Y :   2.8296 V\n",
      "X :     3.447 V     Y :   2.8296 V\n"
     ]
    }
   ],
   "source": [
    "constants = (   5.0,    # V+ (V)\n",
    "             1000.0,    # R1 (ohm)\n",
    "             4000.0,    # R2 (ohm)\n",
    "             3000.0,    # R3 (ohm)\n",
    "             2000.0,    # R4 (ohm)\n",
    "                3.0E-9, # I0 (A)\n",
    "                0.05)   # VT (V)\n",
    "\n",
    "# -- In this example, I am packing the variables I want to calculate into an array.\n",
    "# -- I find this logical, as is should be a vector of solutions.\n",
    "# -- This is, of course, the initial guess.\n",
    "v12 = numpy.array([0.1,  # V1\n",
    "                   0.1]) # V2\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "def voltage_function(v12,args) :\n",
    "    \n",
    "    # -- I am unpacking the varaibles into new varaibles with the same names as the written problem.\n",
    "    # -- I find this makes it easier to enter the equation correctly.\n",
    "    vp,r1,r2,r3,r4,i0,vt = args\n",
    "    v1,v2 = v12\n",
    "    \n",
    "    temp = i0*(numpy.exp((v1-v2)/vt)-1.0)\n",
    "    f0 = (v1-vp)/r1 + v1/r2 + temp\n",
    "    f1 = (v2-vp)/r3 + v2/r4 - temp\n",
    "    \n",
    "    # -- Since the variables come in as an array, that is how I will return them.\n",
    "    return numpy.array([f0,f1])\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "def voltage_jacobian(v12,args) :\n",
    "\n",
    "    vp,r1,r2,r3,r4,i0,vt = args\n",
    "    v1,v2 = v12\n",
    "    \n",
    "    # -- This Jacobian is a 2x2 matrix (see the return statement below)\n",
    "    # -- Watch out here, indexing by zero is not how we wrote the problem above.\n",
    "    temp = i0*(numpy.exp((v1-v2)/vt))/vt\n",
    "    J00 = 1./r1 + 1./r2 + temp\n",
    "    J01 = -temp\n",
    "    J10 = -temp\n",
    "    J11 = 1./r3 + 1./r4 + temp\n",
    "    \n",
    "    return numpy.array([[J00,J01],[J10,J11]])\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    # -- In each step, we need to find delta-V from the function f and J\n",
    "    # -- Having them in a vector/matrix format works great for numpy.linalg.solve.\n",
    "    f = voltage_function(v12, constants)\n",
    "    J = voltage_jacobian(v12, constants)\n",
    "    dv = numpy.linalg.solve(J,f)\n",
    "    # -- Then we step towards the solutions\n",
    "    v12p = v12 - dv\n",
    "    if numpy.allclose(v12p, v12, atol=1.E-8,rtol=0.):\n",
    "        print(\"X : {0:9.5g} V     Y : {1:8.5g} V\".format(v12[0],v12[1]))\n",
    "        break\n",
    "    else :\n",
    "        v12 = v12p\n",
    "        print(\"X : {0:9.5g} V     Y : {1:8.5g} V\".format(v12[0],v12[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2 continued\n",
    "In <a href=\"https://docs.scipy.org/doc/scipy/reference/optimize.html\">`scipy.optimize`</a> there are an amazing array of routines for solving nonlinear equations an root finding. This problem is <a href=\"https://docs.scipy.org/doc/scipy/reference/optimize.html#multidimensional\">multidimensional root finding</a>, so we should use <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.root.html#scipy.optimize.root\">`root`</a>. There are methods that use the Jacobian, and many that do not. Use one of each kind to verify the answer above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYBR method message :  The solution converged.\n",
      "[3.44695462 2.82956807]\n",
      "dict_keys(['x', 'message', 'status', 'success', 'cov_x', 'fun', 'nfev', 'njev', 'fjac', 'ipvt', 'qtf'])\n",
      "LM method message :  The relative error between two consecutive iterates is at most 0.000000\n",
      "[3.44695462 2.82956807]\n"
     ]
    }
   ],
   "source": [
    "constants = (   5.0,    # V+ (V)\n",
    "             1000.0,    # R1 (ohm)\n",
    "             4000.0,    # R2 (ohm)\n",
    "             3000.0,    # R3 (ohm)\n",
    "             2000.0,    # R4 (ohm)\n",
    "                3.0E-9, # I0 (A)\n",
    "                0.05)   # VT (V)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "def voltage_function(v12,*args) :\n",
    "    \n",
    "    vp,r1,r2,r3,r4,i0,vt = args\n",
    "    v1,v2 = v12\n",
    "    \n",
    "    temp = i0*(numpy.exp((v1-v2)/vt)-1.0)\n",
    "    f0 = (v1-vp)/r1 + v1/r2 + temp\n",
    "    f1 = (v2-vp)/r3 + v2/r4 - temp\n",
    "    \n",
    "    return numpy.array([f0,f1])\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "def voltage_jacobian(v12,*args) :\n",
    "\n",
    "    vp,r1,r2,r3,r4,i0,vt = args\n",
    "    v1,v2 = v12\n",
    "    \n",
    "    temp = i0*(numpy.exp((v1-v2)/vt))/vt\n",
    "    J00 = 1./r1 + 1./r2 + temp\n",
    "    J01 = -temp\n",
    "    J10 = -temp\n",
    "    J11 = 1./r3 + 1./r4 + temp\n",
    "    \n",
    "    return numpy.array([[J00,J01],[J10,J11]])\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -- The first method we will use is the non-Jacobian method 'hybr'\n",
    "# -- Find the roots of a multivariate function using MINPACKâ€™s hybrd and hybrj routines (modified Powell method).\n",
    "# -- https://en.wikipedia.org/wiki/MINPACK\n",
    "v12 = numpy.array((3.0,  # V1\n",
    "                   2.0)) # V2\n",
    "res = scipy.optimize.root(voltage_function,v12,args=constants,method='hybr')\n",
    "print(\"HYBR method message : \",res.message)\n",
    "print(res.x)\n",
    "# -- The second method we will use is the method 'lm' which needs the jacobian\n",
    "# -- Solve for least squares with Levenberg-Marquardt\n",
    "# -- https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm\n",
    "v12 = numpy.array((3.0,  # V1\n",
    "                   3.0)) # V2\n",
    "res = scipy.optimize.root(voltage_function,v12,args=constants,jac=voltage_jacobian,method='lm')\n",
    "# print(res.keys())\n",
    "print(\"LM method message : \",res.message)\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "Mathematical optimization deals with the problem of numerically finding minimums (or maximums, or zeros) of a function. In this context, the function we wish to find the maximum of is called *cost function*, or sometimes the *objective function*, or in some cases the *energy*.\n",
    "\n",
    "Here, we are interested in using [`scipy.optimize`][1] as a black-box. We are not going into the details of how the different methods work, but we should still learn to appriciate their differences because not all optimization problems are equal. Knowing your problem enables you to choose the right tool. Sometimes having a Jacobian will really help as well. Note that Newton's method is pretty much the starting point for many of these methods.\n",
    "\n",
    "Read through this page on [optimization][2] for examples.\n",
    "\n",
    "Let's work through some [`scipy.optimize.minimize`][3] examples and excercises in 1 and 2 dimensions.\n",
    "\n",
    "[1]: https://docs.scipy.org/doc/scipy/reference/optimize.html\n",
    "[2]: http://scipy-lectures.org/advanced/mathematical_optimization/\n",
    "[3]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3    1-D\n",
    "\n",
    "Find the minimum of the equation $f(x)=e^{(x-a)^2}$ for $a=0.7$. Use the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 1.0\n",
      "     jac: array([-7.83642928e-10])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 9\n",
      "    nhev: 0\n",
      "     nit: 9\n",
      "    njev: 17\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.7])\n"
     ]
    }
   ],
   "source": [
    "constant = (0.7,)\n",
    "\n",
    "def f(x,*args) :\n",
    "    return numpy.exp((x[0]-args[0])**2)\n",
    "\n",
    "def J(x, *args) :\n",
    "    t = (x[0]-args[0])\n",
    "    return 2*t*numpy.exp(t**2)\n",
    "\n",
    "guess = 2.5\n",
    "res = scipy.optimize.minimize(f,guess,args=constant,method='Newton-CG',jac=J)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4    2D\n",
    "\n",
    "Find the minimum of the function\n",
    "$$\n",
    "f(x,y) = \\sqrt{(x-a)^2+(y-b)^2}\n",
    "$$ \n",
    "for $a=2$, $b=20$.\n",
    "This is actually a difficult problem, because the Jacobian is poorly behaved near the solution. Solving the same problem for $f^2(x)$ is the same problem with the same solutions. Changing the problem like this is called pre-conditioning.\n",
    "\n",
    "Try finding one solution with constraints on the solutions, keeping them within the bounds of $a=[-3,3]$, and $b=[1.5,12.5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 4.589117562233506\n",
      " hess_inv: array([[1, 0],\n",
      "       [0, 1]])\n",
      "      jac: array([-0.19611614, -0.98058068])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 53\n",
      "      nit: 0\n",
      "     njev: 48\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([ 1.1, 15.5]) \n",
      "\n",
      "      fun: 1.9567661551385958e-09\n",
      " hess_inv: array([[1.53093699e-08, 4.16352467e-10],\n",
      "       [4.16352467e-10, 4.81279046e-09]])\n",
      "      jac: array([0.82943351, 0.74736886])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 254\n",
      "      nit: 7\n",
      "     njev: 80\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([ 2., 20.]) \n",
      "\n",
      " final_simplex: (array([[ 2.00003293, 19.99998227],\n",
      "       [ 1.99996508, 20.00003989],\n",
      "       [ 2.00002588, 19.99995081]]), array([3.74029573e-05, 5.30167465e-05, 5.55816145e-05]))\n",
      "           fun: 3.740295732252786e-05\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 85\n",
      "           nit: 44\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([ 2.00003293, 19.99998227]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "constant = (2.0,20.0)\n",
    "\n",
    "def f(x,*args) :\n",
    "    a,b = args\n",
    "    temp = numpy.sqrt(((x[0]-a)**2)+((x[1]-b)**2))\n",
    "    return temp\n",
    "\n",
    "def J(x, *args) :\n",
    "    a,b = args\n",
    "    temp = numpy.sqrt(((x[0]-a)**2)+((x[1]-b)**2))\n",
    "    Ja = (x[0]-a)/temp\n",
    "    Jb = (x[1]-b)/temp\n",
    "    return numpy.array([Ja,Jb])\n",
    " \n",
    "guess = numpy.array([1.1,15.5])\n",
    "\n",
    "res = scipy.optimize.minimize(f,guess,args=constant,method=\"BFGS\",jac=J)\n",
    "print(res,\"\\n\")\n",
    "res = scipy.optimize.minimize(f,guess,args=constant,method=\"BFGS\")\n",
    "print(res,\"\\n\")\n",
    "res = scipy.optimize.minimize(f,guess,args=constant,method=\"Nelder-Mead\")\n",
    "print(res,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   direc: array([[-6.40841120e-07, -1.26871144e-05],\n",
      "       [ 3.91337640e-13, -3.47732042e-15]])\n",
      "     fun: 0.0\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 358\n",
      "     nit: 7\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 2., 20.]) \n",
      "\n",
      "     fun: 4.589117562233506\n",
      "     jac: array([-0.19611614, -0.98058068])\n",
      " message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "    nfev: 53\n",
      "     nit: 0\n",
      "    njev: 48\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([ 1.1, 15.5]) \n",
      "\n",
      "     fun: 7.500000241639889\n",
      "     jac: array([ 2.53856182e-04, -9.99999940e-01])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 12\n",
      "     nit: 4\n",
      "    njev: 4\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 2.00190384, 12.5       ]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "constant = (2.0,20.0)\n",
    "\n",
    "def f(x,*args) :\n",
    "    a,b = args\n",
    "    temp = numpy.sqrt(((x[0]-a)**2)+((x[1]-b)**2))\n",
    "    return temp\n",
    "\n",
    "def J(x, *args) :\n",
    "    a,b = args\n",
    "    temp = numpy.sqrt(((x[0]-a)**2)+((x[1]-b)**2))\n",
    "    Ja = (x[0]-a)/temp\n",
    "    Jb = (x[1]-b)/temp\n",
    "    return numpy.array([Ja,Jb])\n",
    " \n",
    "guess = numpy.array([1.1,15.5])\n",
    "\n",
    "res = scipy.optimize.minimize(f,guess,args=constant,method=\"POWELL\")\n",
    "print(res,\"\\n\")\n",
    "res = scipy.optimize.minimize(f,guess,args=constant,method=\"CG\",jac=J)\n",
    "print(res,\"\\n\")\n",
    "res = scipy.optimize.minimize(f,guess,args=constant,method=\"SLSQP\",bounds=((-3., 3.), (1.5, 12.5)))\n",
    "print(res,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 1.9721522630525295e-31\n",
      " hess_inv: array([[ 0.98076923, -0.09615385],\n",
      "       [-0.09615385,  0.51923077]])\n",
      "      jac: array([8.8817842e-16, 0.0000000e+00])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 4\n",
      "      nit: 3\n",
      "     njev: 4\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 2., 20.]) \n",
      "\n",
      "      fun: 8.19105207292833e-14\n",
      " hess_inv: array([[ 0.98076921, -0.09615389],\n",
      "       [-0.09615389,  0.51923078]])\n",
      "      jac: array([-5.53844051e-07,  7.94841810e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 12\n",
      "      nit: 3\n",
      "     njev: 4\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 1.99999972, 20.00000003]) \n",
      "\n",
      " final_simplex: (array([[ 2.00003293, 19.99998227],\n",
      "       [ 1.99996508, 20.00003989],\n",
      "       [ 2.00002588, 19.99995081]]), array([1.39898122e-09, 2.81077541e-09, 3.08931587e-09]))\n",
      "           fun: 1.3989812164708407e-09\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 85\n",
      "           nit: 44\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([ 2.00003293, 19.99998227]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "constant = (2.0,20.0)\n",
    "\n",
    "def f(x,*args) :\n",
    "    a,b = args\n",
    "    temp = ((x[0]-a)**2)+((x[1]-b)**2)\n",
    "    return temp\n",
    "\n",
    "def J(x, *args) :\n",
    "    a,b = args\n",
    "    temp = ((x[0]-a)**2)+((x[1]-b)**2)\n",
    "    Ja = 2.0*(x[0]-a)\n",
    "    Jb = 2.0*(x[1]-b)\n",
    "    return numpy.array([Ja,Jb])\n",
    " \n",
    "guess = numpy.array([1.1,15.5])\n",
    "\n",
    "res = scipy.optimize.minimize(f,guess,args=constant,method=\"BFGS\",jac=J)\n",
    "print(res,\"\\n\")\n",
    "res = scipy.optimize.minimize(f,guess,args=constant,method=\"BFGS\")\n",
    "print(res,\"\\n\")\n",
    "res = scipy.optimize.minimize(f,guess,args=constant,method=\"Nelder-Mead\")\n",
    "print(res,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97f613484c7082a5f441d3814b62bbd2ba6367090f977f8bd3f34e33dc661583"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
